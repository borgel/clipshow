{"id":"clipshow-052","title":"Step 7: Detection Pipeline","description":"Implement detection/pipeline.py: orchestrate running all enabled detectors on each video source. Respect detector weights (skip weight=0). Pass progress callbacks and cancellation flags. Collect DetectorResults and delegate to scoring.py for segment extraction. Write test_pipeline.py with both mock detectors and real synthetic video E2E. Run testreview-gemini after.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:27:31Z","closed_at":"2026-02-23T22:27:31Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-052","depends_on_id":"clipshow-0i5","type":"blocks","created_at":"2026-02-23T14:00:28Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-052","depends_on_id":"clipshow-nr3","type":"blocks","created_at":"2026-02-23T14:00:28Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-052","depends_on_id":"clipshow-o5b","type":"blocks","created_at":"2026-02-23T14:00:28Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-0i5","title":"Step 6: Motion Detector","description":"Implement detection/motion.py: OpenCV absdiff on decimated grayscale frames to detect motion/action. Implement Detector interface. Return DetectorResult with normalized scores. Add tests to test_detectors.py verifying motion_video scores higher than static_video.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:25:55Z","closed_at":"2026-02-23T22:25:55Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-0i5","depends_on_id":"clipshow-144","type":"blocks","created_at":"2026-02-23T14:00:27Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-0jk","title":"Fix macOS CI segfault in QMediaPlayer teardown","description":"macOS CI segfaults during pytest teardown of ReviewPanel tests. QMediaPlayer tries to open non-existent video files and crashes during widget destruction. Need proper cleanup in VideoPreview (stop player, clear source before destruction) and error handling for missing files.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T00:23:24Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T00:34:44Z","closed_at":"2026-02-24T00:34:44Z","close_reason":"Closed"}
{"id":"clipshow-13z","title":"Add pytest-xdist for parallel test execution","description":"Add pytest-xdist dependency and -n auto flag to CI and local test runs for faster test execution using all available CPU cores.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T02:43:02Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T02:43:29Z","closed_at":"2026-02-24T02:43:29Z","close_reason":"Closed"}
{"id":"clipshow-144","title":"Step 3: Scoring Math","description":"Implement detection/scoring.py: pure-numpy scoring engine. Resample detector score arrays to common 10 samples/sec time base. Weighted combination of enabled detectors, renormalize to [0,1]. Threshold to find interesting timesteps, extract contiguous runs as candidate segments, apply pre/post padding (clamp to boundaries), merge overlapping segments (gap \u003c0.5s), filter by min/max duration, rank by peak score. Write test_scoring.py. Run testreview-gemini after.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:19:16Z","closed_at":"2026-02-23T22:19:16Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-144","depends_on_id":"clipshow-9rm","type":"blocks","created_at":"2026-02-23T14:00:27Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-1qv","title":"Include semantic detection in release builds (lite + full variants)","description":"Release builds currently miss semantic detection because onnx_clip/onnxruntime aren't installed. Add 'all' extra to pyproject.toml, update release.yml with matrix for lite (models downloaded on first use) and full (577MB models bundled) variants per platform, update PyInstaller specs with semantic hiddenimports and conditional model bundling, update Flatpak manifest to install [all].","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:12:06Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:21:44Z","closed_at":"2026-02-24T21:21:44Z","close_reason":"Closed"}
{"id":"clipshow-1xe","title":"Feature 24: Linux Flatpak Packaging","description":"Create Flatpak manifest, desktop entry, metainfo. Update release.yml with build-linux job.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-23T23:24:58Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-23T23:30:20Z","closed_at":"2026-02-23T23:30:20Z","close_reason":"Closed"}
{"id":"clipshow-21q","title":"Feature 22: Settings Dialog","description":"Create settings_dialog.py with all settings groups. Add Edit \u003e Preferences menu to main_window.py. Write tests.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-23T23:24:55Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-23T23:28:54Z","closed_at":"2026-02-23T23:28:54Z","close_reason":"Closed"}
{"id":"clipshow-2gc","title":"Fix visual regression tests failing cross-platform in CI","description":"Visual regression tests fail on Windows (SSIM 0.66-0.89) and have reshape errors because baselines were generated on a different platform. Tests need to be platform-aware: skip in CI or generate per-platform baselines.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T00:23:22Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T00:34:44Z","closed_at":"2026-02-24T00:34:44Z","close_reason":"Closed"}
{"id":"clipshow-3fc","title":"Step 15: Export Panel","description":"Implement ui/export_panel.py: output path picker, resolution/fps/bitrate settings, summary (N segments, total duration), export button + progress bar. Write workers/export_worker.py (QThread). Write test_ui_export.py verifying settings binding, worker completion flow, progress bar updates.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:46:32Z","closed_at":"2026-02-23T22:46:32Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-3fc","depends_on_id":"clipshow-8eq","type":"blocks","created_at":"2026-02-23T14:00:31Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-3fc","depends_on_id":"clipshow-bhm","type":"blocks","created_at":"2026-02-23T14:00:32Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-3uw","title":"Update README with application screenshots","description":"Add screenshots of the ClipShow UI to the README to give users a visual preview of the application workflow (Import, Analyze, Review, Export panels).","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T20:41:43Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:36:46Z","closed_at":"2026-02-24T21:36:46Z","close_reason":"Closed"}
{"id":"clipshow-55f","title":"Release v0.4.0 with semantic detection in builds","description":"After all release build changes are complete: bump version to 0.4.0, tag, push, verify all 5 release jobs pass (macOS lite/full, Windows lite/full, Linux), verify GitHub Release has all artifacts.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:12:14Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:49:39Z","closed_at":"2026-02-24T21:49:39Z","close_reason":"Closed"}
{"id":"clipshow-5si","title":"Step 18: Emotion Detector","description":"Implement detection/emotion.py: MediaPipe face detection + deepface-onnx emotion classification. Lazy-load on first use. Sample at 3 FPS. Score higher for positive/high-energy emotions (happy, surprise). Show install prompt if deps missing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T23:01:34Z","closed_at":"2026-02-23T23:01:34Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-5si","depends_on_id":"clipshow-052","type":"blocks","created_at":"2026-02-23T14:00:33Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-5us","title":"Plan: substantially improve semantic analysis quality and performance","description":"Write and document a plan for major improvements to the semantic detector. The current implementation uses CLIP ViT-B/32 at 2 FPS with single-frame scoring. This bead tracks a comprehensive improvement plan covering the areas below.\n\n## Current Limitations\n- CLIP ViT-B/32 is a small, older model with limited visual understanding\n- Frame-by-frame processing with zero temporal context (can't detect actions/events)\n- 2 FPS sampling misses brief moments; no batching (slow)\n- Single full-frame embedding — no attention to regions of interest\n- Naive text prompts with no prompt engineering or ensemble techniques\n\n## Proposed Improvement Areas\n\n### 1. Model Upgrade: SigLIP or CLIP ViT-L/14\nReplace ViT-B/32 (~338MB) with a stronger vision-language model:\n- **SigLIP ViT-B/16** (~400MB ONNX) — significantly better zero-shot accuracy than CLIP ViT-B/32, trained with sigmoid loss instead of contrastive, better calibrated scores\n- **CLIP ViT-L/14** (~900MB ONNX) — much larger receptive field and embedding space, best open CLIP variant\n- Keep ViT-B/32 as a \"lite\" fallback for users who want smaller downloads\n- Benchmark: SigLIP ViT-B/16 gets ~70% ImageNet zero-shot vs CLIP ViT-B/32's ~63%\n\n### 2. Temporal Context: Video-Aware Scoring\nCurrent approach scores each frame independently. Improvements:\n- **Sliding window aggregation** — score groups of 3-5 frames and combine embeddings (mean pool) before comparing to text, capturing short actions\n- **Keyframe + context** — for each sampled frame, also sample ±0.5s neighbors and use the max/mean of the group\n- **Temporal attention** — lightweight temporal transformer on top of per-frame CLIP embeddings to model short-range video dynamics (would need a small custom ONNX model)\n- **Optional: X-CLIP or VideoCLIP** — purpose-built video-language models that understand temporal dynamics, though much heavier\n\n### 3. Multi-Scale / Region Crops\nThe current approach embeds the full frame only. Adding region crops can catch small but important subjects:\n- **Center crop + full frame** — embed both and take the max similarity\n- **Adaptive crops** — use a lightweight object detector or saliency map to find regions of interest, then embed those crops separately\n- **Multi-resolution** — process at 224px (standard CLIP) and 336px (higher detail) and combine\n\n### 4. Batch Processing \u0026 GPU Acceleration\nCurrent approach processes 1 frame at a time on CPU:\n- **Batch inference** — collect N frames (e.g., 8-16) and run ONNX inference in a single batch, dramatically reducing per-frame overhead\n- **ONNX GPU provider** — use CUDAExecutionProvider or CoreMLExecutionProvider when available, with automatic fallback to CPU\n- **Async frame decoding** — decode frames in a separate thread while inference runs on the current batch\n\n### 5. Prompt Engineering \u0026 Ensemble\nCurrent prompts are single sentences. Better approaches:\n- **Prompt templates** — use ensemble of templates like \"a photo of {}\", \"a video frame showing {}\", \"a screenshot of {}\" and average their text embeddings (this is what OpenAI recommends for CLIP)\n- **Hierarchical prompts** — break user prompts into categories (action, object, scene, emotion) and weight them differently\n- **Auto-generated negative prompts** — given positive prompts, automatically generate contrasting negatives using antonyms or LLM suggestions\n\n### 6. Adaptive Sampling\nInstead of fixed 2 FPS:\n- **Two-pass approach** — first pass at 1 FPS with cheap features (histogram diff, motion magnitude) to identify \"interesting\" regions, then second pass at 4-8 FPS with full CLIP only on those regions\n- **Score-guided sampling** — if a frame scores high, increase sampling rate in that neighborhood to find precise boundaries\n\n## Implementation Priority\n1. Batch processing (biggest speed win, easy)\n2. Model upgrade to SigLIP (biggest quality win, moderate effort)\n3. Prompt ensemble (free quality win, easy)\n4. Adaptive sampling (speed + quality, moderate)\n5. Multi-scale crops (quality win, moderate)\n6. Temporal context (quality win for action detection, harder)","status":"open","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:51:39Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:51:39Z"}
{"id":"clipshow-5w5","title":"Surface skipped detector warnings and install onnx_clip dependency","description":"The semantic detector silently skips when onnx_clip is missing. Two fixes: 1) Add user-visible feedback when a detector is skipped due to missing dependency. 2) Install onnx_clip and onnxruntime as project dependencies.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T05:08:09Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T05:11:32Z","closed_at":"2026-02-24T05:11:32Z","close_reason":"Closed"}
{"id":"clipshow-6w1","title":"Add temporal smoothing to semantic scores","description":"A brief high-scoring frame surrounded by low ones is likely noise. Apply a small smoothing window to semantic scores to reduce false positives.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:04:08Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:24:29Z","closed_at":"2026-02-24T19:24:29Z","close_reason":"Closed"}
{"id":"clipshow-6yk","title":"Fix stale version strings in packaging files","description":"packaging/clipshow_macos.spec has CFBundleShortVersionString=0.1.0 and packaging/installer.iss has AppVersion=0.1.0 and OutputBaseFilename=ClipShow-0.1.0-setup. Update all to 0.3.0.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:12:07Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:21:45Z","closed_at":"2026-02-24T21:21:45Z","close_reason":"Closed"}
{"id":"clipshow-7sh","title":"Step 12: Video Preview","description":"Implement ui/video_preview.py: QMediaPlayer + QVideoWidget wrapper for video playback. Play segment at given start/end time. Play/pause controls. Mock QMediaPlayer in tests to avoid actual playback.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:41:13Z","closed_at":"2026-02-23T22:41:13Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-7sh","depends_on_id":"clipshow-haj","type":"blocks","created_at":"2026-02-23T14:00:30Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-8eq","title":"Step 14: Review Panel","description":"Implement ui/review_panel.py: left side draggable segment list with include/exclude checkboxes, right side video preview widget. Bottom trim controls with +/-0.5s nudge buttons. Click segment to preview. Implement ui/segment_list.py and ui/timeline_widget.py. Write test_ui_review.py verifying reorder, trim, include/exclude, preview integration.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:44:47Z","closed_at":"2026-02-23T22:44:47Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-8eq","depends_on_id":"clipshow-mtn","type":"blocks","created_at":"2026-02-23T14:00:31Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-8v0","title":"Step 17: Semantic Detector","description":"Implement detection/semantic.py: CLIP-based content scoring via onnx_clip (ONNX Runtime, no PyTorch). Lazy-load on first use with importlib.import_module(). Auto-download CLIP ViT-B/32 model (338MB) to ~/.clipshow/models/. Sample frames at 1 FPS, score against user text prompts, return cosine similarity. Show install/download prompt if deps missing.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T23:01:33Z","closed_at":"2026-02-23T23:01:33Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-8v0","depends_on_id":"clipshow-052","type":"blocks","created_at":"2026-02-23T14:00:32Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-8zn","title":"Step 9: Auto Mode","description":"Wire up run_auto_mode() in app.py: load files, run ffprobe for metadata, run detection pipeline with default settings, take all segments above threshold sorted chronologically, assemble via MoviePy, write output. Support --headless (no GUI) and minimal progress window. Write test_auto_mode.py as full E2E with synthetic videos.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:54:13Z","closed_at":"2026-02-23T22:54:13Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-8zn","depends_on_id":"clipshow-052","type":"blocks","created_at":"2026-02-23T14:00:29Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-8zn","depends_on_id":"clipshow-bhm","type":"blocks","created_at":"2026-02-23T14:00:29Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-9ck","title":"Step 19: Visual Regression Baselines","description":"Regenerate test_ui_layout.py visual regression baselines for all panels. Capture screenshots at fixed 800x600 via widget.grab(). Compare with scikit-image SSIM (threshold 0.95). Support --update-baselines flag. Save diff screenshots as artifacts on CI failure. Cover: import (empty/loaded), analyze (default/custom), review (with segments), export (default/progress), main window.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T23:04:24Z","closed_at":"2026-02-23T23:04:24Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-9ck","depends_on_id":"clipshow-bir","type":"blocks","created_at":"2026-02-23T14:00:33Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-9rm","title":"Step 1: Project Scaffold","description":"- type: task","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:11:53Z","closed_at":"2026-02-23T22:11:53Z","close_reason":"Closed"}
{"id":"clipshow-9tm","title":"Update README to explain lite vs full release builds","description":"Add a section to README explaining the two release build variants: lite (smaller download, downloads CLIP models on first use, needs internet) vs full (larger ~800MB, works offline immediately). Explain why the full builds are large (577MB CLIP ViT-B/32 ONNX model).","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:12:13Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:36:46Z","closed_at":"2026-02-24T21:36:46Z","close_reason":"Closed"}
{"id":"clipshow-ad8","title":"E2E UI test with real fixture videos","description":"Write automated test that drives MainWindow through full workflow (import, analyze, review, export) using real video fixtures from tests/fixtures/videos/","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-23T23:55:00Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T00:01:41Z","closed_at":"2026-02-24T00:01:41Z","close_reason":"Closed"}
{"id":"clipshow-age","title":"Fix Ubuntu CI: missing libEGL.so.1 for Qt offscreen mode","description":"Ubuntu CI fails with INTERNALERROR: ImportError: libEGL.so.1: cannot open shared object file. Need to install libegl1-mesa in the apt-get step of ci.yml.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T00:23:20Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T00:34:43Z","closed_at":"2026-02-24T00:34:43Z","close_reason":"Closed"}
{"id":"clipshow-bgw","title":"Slider help text in analyze panel","description":"Add explanatory help text below detector weights and threshold sliders","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T03:58:25Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T03:59:24Z","closed_at":"2026-02-24T03:59:24Z","close_reason":"Closed"}
{"id":"clipshow-bhm","title":"Step 8: Video Assembler","description":"Implement export/assembler.py: use MoviePy 2.x to concatenate video subclips and encode to output MP4 (H.264). Support subclip start/end times. Hard cuts only (no crossfades). Write test_assembler.py verifying output exists, correct duration, valid codec. Verify via ffprobe.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:29:26Z","closed_at":"2026-02-23T22:29:26Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-bhm","depends_on_id":"clipshow-ocm","type":"blocks","created_at":"2026-02-23T14:00:29Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-bir","title":"Step 16: Full UI E2E","description":"Write test_ui_workflow.py: full end-to-end UI test simulating Import -\u003e Analyze -\u003e Review -\u003e Export journey with qtbot. Programmatically add test videos, trigger analysis, verify segments, toggle/reorder segments, export to temp file, verify valid MP4 output. Run testreview-gemini on all UI tests.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:51:38Z","closed_at":"2026-02-23T22:51:38Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-bir","depends_on_id":"clipshow-3fc","type":"blocks","created_at":"2026-02-23T14:00:32Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-bjn","title":"Show detector category in review panel","description":"Add detectors field to HighlightSegment and show detector tags in segment list","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T03:58:34Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:01:50Z","closed_at":"2026-02-24T04:01:50Z","close_reason":"Closed"}
{"id":"clipshow-di1","title":"Step 20: Packaging \u0026 CI/CD","description":"Create PyInstaller specs (packaging/clipshow_macos.spec, clipshow_windows.spec), Inno Setup script (packaging/installer.iss), release.yml GitHub Action (triggered on version tags). macOS: .app bundle + .dmg via create-dmg. Windows: folder-mode .exe + Inno installer. Both: hiddenimports for PySide6 multimedia + scenedetect submodules. Exclude tkinter/matplotlib.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T23:05:32Z","closed_at":"2026-02-23T23:05:32Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-di1","depends_on_id":"clipshow-bir","type":"blocks","created_at":"2026-02-23T14:00:33Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-dw7","title":"Increase semantic detector sample rate to 2 FPS","description":"Current 1 FPS sampling misses brief highlights (\u003c 1s). Bump SAMPLE_FPS to 2 for better coverage at modest speed cost.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:04:08Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:24:29Z","closed_at":"2026-02-24T19:24:29Z","close_reason":"Closed"}
{"id":"clipshow-er5","title":"Feature 21: Semantic Prompts + Pipeline Bug Fix","description":"Fix semantic_prompts not passed to SemanticDetector in pipeline.py. Create PromptEditor widget. Add semantic/emotion rows to AnalyzePanel. Write tests.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-23T23:24:54Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-23T23:27:05Z","closed_at":"2026-02-23T23:27:05Z","close_reason":"Closed"}
{"id":"clipshow-haj","title":"Step 10: UI Shell","description":"Implement ui/main_window.py: 4-tab workflow (Import, Analyze, Review, Export) with sequential tab enabling. Navigation buttons (Back/Next). Write test_ui_layout.py baseline screenshots. Verify PySide6 launches headlessly with QT_QPA_PLATFORM=offscreen.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:29:27Z","closed_at":"2026-02-23T22:29:27Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-haj","depends_on_id":"clipshow-9rm","type":"blocks","created_at":"2026-02-23T14:00:29Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-hf4","title":"Add FFmpeg thread parallelism flags","description":"Add -threads 0 to FFmpeg subprocess calls in audio extraction and MoviePy write_videofile to enable multi-threaded encoding/decoding","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T01:16:27Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T01:16:39Z","closed_at":"2026-02-24T01:16:39Z","close_reason":"Closed"}
{"id":"clipshow-iov","title":"Sort segments by filename in main window","description":"Sort segments by (source_path, start_time) in _on_analysis_complete to match CLI behavior","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T03:58:35Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:01:30Z","closed_at":"2026-02-24T04:01:30Z","close_reason":"Closed"}
{"id":"clipshow-jlm","title":"Fix README cloning instructions referencing 'your-org'","description":"README mentions 'your-org' instead of the actual repo path in cloning instructions. GitHub issue #2.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:04:07Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:05:37Z","closed_at":"2026-02-24T19:05:37Z","close_reason":"Closed"}
{"id":"clipshow-jrn","title":"Replace segment list with table view in review panel","description":"Replace QListWidget with QTableWidget for cleaner columnar display of segment data","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T04:35:12Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:37:23Z","closed_at":"2026-02-24T04:37:23Z","close_reason":"Closed"}
{"id":"clipshow-k94","title":"Better CLIP similarity normalization","description":"Use sigmoid-based normalization centered on typical CLIP match threshold instead of linear (score+1)/2 rescaling. Current approach squashes everything into a narrow band around 0.55-0.68, losing dynamic range between real matches and noise.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:04:07Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:24:28Z","closed_at":"2026-02-24T19:24:28Z","close_reason":"Closed"}
{"id":"clipshow-l78","title":"Block Next until analysis completes","description":"Disable Next button on Analyze tab until analysis produces results","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T03:58:34Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:01:30Z","closed_at":"2026-02-24T04:01:30Z","close_reason":"Closed"}
{"id":"clipshow-lns","title":"Feature 23: README","description":"Write comprehensive README.md for novice users covering all features.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-23T23:24:56Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-23T23:29:33Z","closed_at":"2026-02-23T23:29:33Z","close_reason":"Closed"}
{"id":"clipshow-mtn","title":"Step 13: Analyze Panel","description":"Implement ui/analyze_panel.py: sliders for detector weights and threshold, enable/disable checkboxes per detector. Semantic detectors show model download note. Per-file progress bars. Analyze All button launches AnalysisWorker (QThread). Write workers/analysis_worker.py. Write test_ui_analyze.py verifying slider binding, worker signals, progress, cancel.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:42:59Z","closed_at":"2026-02-23T22:42:59Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-mtn","depends_on_id":"clipshow-052","type":"blocks","created_at":"2026-02-23T14:00:31Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-mtn","depends_on_id":"clipshow-7sh","type":"blocks","created_at":"2026-02-23T14:00:31Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-mtn","depends_on_id":"clipshow-uzs","type":"blocks","created_at":"2026-02-23T14:00:30Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-n8b","title":"Update README to promote release builds over pip install","description":"Adjust the README installation section to direct users towards downloading pre-built release builds (DMG, Windows installer, Flatpak) as the primary install method, with pip/uv install as a secondary developer option.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T20:41:45Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T20:43:26Z","closed_at":"2026-02-24T20:43:26Z","close_reason":"Closed"}
{"id":"clipshow-noq","title":"Parallel video analysis with ThreadPoolExecutor","description":"Use ThreadPoolExecutor in AnalysisWorker to analyze multiple videos concurrently, keeping granular per-file progress","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T04:44:56Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:49:32Z","closed_at":"2026-02-24T04:49:32Z","close_reason":"Closed"}
{"id":"clipshow-nr3","title":"Step 4: Scene Detector","description":"Implement detection/scene.py: wrap PySceneDetect ContentDetector for HSV-weighted frame differencing. Implement Detector interface from base.py. Return DetectorResult with normalized [0,1] scores per timestep. Spikes at cuts/transitions. Add tests to test_detectors.py verifying scene change detection on the scene_change_video fixture.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:25:54Z","closed_at":"2026-02-23T22:25:54Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-nr3","depends_on_id":"clipshow-144","type":"blocks","created_at":"2026-02-23T14:00:27Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-o5b","title":"Step 5: Audio Detector","description":"Implement detection/audio.py: extract audio to temp WAV via FFmpeg subprocess, run librosa onset strength + RMS energy analysis. Implement Detector interface. Return DetectorResult with normalized scores capturing loud moments, speech, beats. Add tests to test_detectors.py verifying detection on loud_moment_video fixture.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:25:54Z","closed_at":"2026-02-23T22:25:54Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-o5b","depends_on_id":"clipshow-144","type":"blocks","created_at":"2026-02-23T14:00:27Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-ocm","title":"Step 2: ffprobe Module","description":"Implement export/ffprobe.py: extract video metadata (duration, width, height, fps, codec) by calling ffprobe as a subprocess. Parse JSON output. Populate VideoSource fields. Write test_ffprobe.py verifying extraction on synthetic test videos from conftest.py.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:19:16Z","closed_at":"2026-02-23T22:19:16Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-ocm","depends_on_id":"clipshow-9rm","type":"blocks","created_at":"2026-02-23T14:00:26Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-qnk","title":"Processing rate, ETA, and frame preview during analysis","description":"Add rate/ETA display and frame preview thumbnail during video analysis","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T03:58:29Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:00:58Z","closed_at":"2026-02-24T04:00:58Z","close_reason":"Closed"}
{"id":"clipshow-tah","title":"Default all analysis detectors to unchecked in UI","description":"Change the Analyze panel so all detector checkboxes start unchecked (weight=0) by default, requiring users to explicitly enable the detectors they want. Currently scene/audio/motion/emotion are enabled by default which can be confusing for new users.","status":"open","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:50:56Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:53:02Z"}
{"id":"clipshow-uyp","title":"Expose negative prompts in UI with clear-all button","description":"Add negative prompts list to the Edit Prompts dialog alongside positive prompts. Add a Clear All button that empties both lists. Add semantic_negative_prompts field to Settings.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:17:05Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:27:31Z","closed_at":"2026-02-24T19:27:31Z","close_reason":"Closed"}
{"id":"clipshow-uzs","title":"Step 11: Import Panel","description":"Implement ui/import_panel.py: drag-drop area + file browser button. File list showing filename, duration, resolution. Remove/clear buttons. Next button disabled when no files. Write test_ui_import.py verifying add/remove/clear flows and button states.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:40:04Z","closed_at":"2026-02-23T22:40:04Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-uzs","depends_on_id":"clipshow-haj","type":"blocks","created_at":"2026-02-23T14:00:30Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-v5c","title":"Fix GH release build failures (macOS + Linux)","description":"macOS: PyInstaller fails with 'not a fat binary' because target_arch=universal2 but numpy is arm64-only. Fix: change target_arch to None (native). Linux: Flatpak fails because pip --no-index --find-links=vendor has no vendor dir with setuptools. Fix: allow network access during build or bundle deps.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:28:25Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T20:00:11Z","closed_at":"2026-02-24T20:00:11Z","close_reason":"Closed"}
{"id":"clipshow-vsp","title":"Add auto-balance toggle for detector weights","description":"Add a toggle on the analysis screen that enables a mode where enabled detector weights automatically divide evenly from 100%. When one detector is enabled it defaults to 100%, two detectors default to 50% each, three to ~33% each, etc. When a detector is toggled on/off in this mode, the remaining enabled detectors should automatically rebalance.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T04:48:40Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:51:06Z","closed_at":"2026-02-24T04:51:06Z","close_reason":"Closed"}
{"id":"clipshow-vyk","title":"Fix scene detector non-deterministic set iteration bug","description":"SceneDetector iterated over StatsManager metric_keys (a set) to find the score key. Due to Python hash randomization, different processes pick different keys from the set. If delta_hue is picked instead of content_val, scores are all-zero for black→white transitions. Fixed by using ContentDetector.FRAME_SCORE_KEY directly.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T02:43:00Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T02:43:29Z","closed_at":"2026-02-24T02:43:29Z","close_reason":"Closed"}
{"id":"clipshow-wb2","title":"Parallel video processing: add --workers flag and ProcessPoolExecutor","description":"Add max_workers to Settings, parallelize analyze_all() with ProcessPoolExecutor, add --workers/-j CLI flag, update settings dialog, and add tests.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-23T23:48:48Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-23T23:52:27Z","closed_at":"2026-02-23T23:52:27Z","close_reason":"Closed"}
{"id":"clipshow-wd0","title":"Add YAML pipeline configuration file support for batch processing","description":"Allow users to pass a YAML config file (e.g. --config pipeline.yaml) to configure the full detection pipeline: detector weights, thresholds, semantic prompts, output settings, input file globs, etc. Useful for repeatable batch processing. Include YAML format documentation in the README with a complete example.","status":"open","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:50:58Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:50:58Z"}
{"id":"clipshow-wkp","title":"Rework analyze panel layout with splitter and file status list","description":"Use QSplitter for settings vs progress area, replace frame preview with file list showing per-clip processing status","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T04:32:34Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:34:19Z","closed_at":"2026-02-24T04:34:19Z","close_reason":"Closed"}
{"id":"clipshow-xxu","title":"Add negative prompts for semantic detector","description":"Score frames against both positive prompts ('exciting moment') and negative prompts ('boring static shot'), use the difference. This dramatically improves contrast between interesting and uninteresting frames.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:04:08Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:24:29Z","closed_at":"2026-02-24T19:24:29Z","close_reason":"Closed"}
{"id":"clipshow-ylu","title":"Fix emotion detector: broken deepface_onnx and mediapipe API","description":"emotion.py imports deepface_onnx but it's not on PyPI (pyproject.toml wrongly lists deepface which is a different TF-based package). Also mediapipe 0.10.31+ removed mp.solutions API. Needs rewrite: either pin mediapipe\u003c0.10.31 and replace deepface with a working ONNX emotion model, or migrate to new MediaPipe Tasks API.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:12:10Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:35:43Z","closed_at":"2026-02-24T21:35:43Z","close_reason":"Closed"}
{"id":"clipshow-z6x","title":"Fix release process: disable flaky tests, attach partial artifacts","description":"Release CI exits with errors due to flaky tests (MoviePy BrokenPipeError on Windows, intermittent fixture failures). Disable known flaky tests and ensure partial build artifacts are still attached even if one platform fails. GitHub issue #1.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:04:07Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:09:31Z","closed_at":"2026-02-24T19:09:31Z","close_reason":"Closed"}
