{"id":"clipshow-052","title":"Step 7: Detection Pipeline","description":"Implement detection/pipeline.py: orchestrate running all enabled detectors on each video source. Respect detector weights (skip weight=0). Pass progress callbacks and cancellation flags. Collect DetectorResults and delegate to scoring.py for segment extraction. Write test_pipeline.py with both mock detectors and real synthetic video E2E. Run testreview-gemini after.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:27:31Z","closed_at":"2026-02-23T22:27:31Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-052","depends_on_id":"clipshow-0i5","type":"blocks","created_at":"2026-02-23T14:00:28Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-052","depends_on_id":"clipshow-nr3","type":"blocks","created_at":"2026-02-23T14:00:28Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-052","depends_on_id":"clipshow-o5b","type":"blocks","created_at":"2026-02-23T14:00:28Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-0i5","title":"Step 6: Motion Detector","description":"Implement detection/motion.py: OpenCV absdiff on decimated grayscale frames to detect motion/action. Implement Detector interface. Return DetectorResult with normalized scores. Add tests to test_detectors.py verifying motion_video scores higher than static_video.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:25:55Z","closed_at":"2026-02-23T22:25:55Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-0i5","depends_on_id":"clipshow-144","type":"blocks","created_at":"2026-02-23T14:00:27Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-0jk","title":"Fix macOS CI segfault in QMediaPlayer teardown","description":"macOS CI segfaults during pytest teardown of ReviewPanel tests. QMediaPlayer tries to open non-existent video files and crashes during widget destruction. Need proper cleanup in VideoPreview (stop player, clear source before destruction) and error handling for missing files.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T00:23:24Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T00:34:44Z","closed_at":"2026-02-24T00:34:44Z","close_reason":"Closed"}
{"id":"clipshow-13z","title":"Add pytest-xdist for parallel test execution","description":"Add pytest-xdist dependency and -n auto flag to CI and local test runs for faster test execution using all available CPU cores.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T02:43:02Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T02:43:29Z","closed_at":"2026-02-24T02:43:29Z","close_reason":"Closed"}
{"id":"clipshow-144","title":"Step 3: Scoring Math","description":"Implement detection/scoring.py: pure-numpy scoring engine. Resample detector score arrays to common 10 samples/sec time base. Weighted combination of enabled detectors, renormalize to [0,1]. Threshold to find interesting timesteps, extract contiguous runs as candidate segments, apply pre/post padding (clamp to boundaries), merge overlapping segments (gap \u003c0.5s), filter by min/max duration, rank by peak score. Write test_scoring.py. Run testreview-gemini after.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:19:16Z","closed_at":"2026-02-23T22:19:16Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-144","depends_on_id":"clipshow-9rm","type":"blocks","created_at":"2026-02-23T14:00:27Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-1qv","title":"Include semantic detection in release builds (lite + full variants)","description":"Release builds currently miss semantic detection because onnx_clip/onnxruntime aren't installed. Add 'all' extra to pyproject.toml, update release.yml with matrix for lite (models downloaded on first use) and full (577MB models bundled) variants per platform, update PyInstaller specs with semantic hiddenimports and conditional model bundling, update Flatpak manifest to install [all].","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:12:06Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:21:44Z","closed_at":"2026-02-24T21:21:44Z","close_reason":"Closed"}
{"id":"clipshow-1xe","title":"Feature 24: Linux Flatpak Packaging","description":"Create Flatpak manifest, desktop entry, metainfo. Update release.yml with build-linux job.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-23T23:24:58Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-23T23:30:20Z","closed_at":"2026-02-23T23:30:20Z","close_reason":"Closed"}
{"id":"clipshow-21q","title":"Feature 22: Settings Dialog","description":"Create settings_dialog.py with all settings groups. Add Edit \u003e Preferences menu to main_window.py. Write tests.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-23T23:24:55Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-23T23:28:54Z","closed_at":"2026-02-23T23:28:54Z","close_reason":"Closed"}
{"id":"clipshow-2gc","title":"Fix visual regression tests failing cross-platform in CI","description":"Visual regression tests fail on Windows (SSIM 0.66-0.89) and have reshape errors because baselines were generated on a different platform. Tests need to be platform-aware: skip in CI or generate per-platform baselines.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T00:23:22Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T00:34:44Z","closed_at":"2026-02-24T00:34:44Z","close_reason":"Closed"}
{"id":"clipshow-3fc","title":"Step 15: Export Panel","description":"Implement ui/export_panel.py: output path picker, resolution/fps/bitrate settings, summary (N segments, total duration), export button + progress bar. Write workers/export_worker.py (QThread). Write test_ui_export.py verifying settings binding, worker completion flow, progress bar updates.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:46:32Z","closed_at":"2026-02-23T22:46:32Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-3fc","depends_on_id":"clipshow-8eq","type":"blocks","created_at":"2026-02-23T14:00:31Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-3fc","depends_on_id":"clipshow-bhm","type":"blocks","created_at":"2026-02-23T14:00:32Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-3uw","title":"Update README with application screenshots","description":"Add screenshots of the ClipShow UI to the README to give users a visual preview of the application workflow (Import, Analyze, Review, Export panels).","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T20:41:43Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:36:46Z","closed_at":"2026-02-24T21:36:46Z","close_reason":"Closed"}
{"id":"clipshow-55f","title":"Release v0.4.0 with semantic detection in builds","description":"After all release build changes are complete: bump version to 0.4.0, tag, push, verify all 5 release jobs pass (macOS lite/full, Windows lite/full, Linux), verify GitHub Release has all artifacts.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:12:14Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:49:39Z","closed_at":"2026-02-24T21:49:39Z","close_reason":"Closed"}
{"id":"clipshow-5cw","title":"Verify CI and release builds with audiovisual optional dependency","description":"Ensure CI and release workflows handle the new audiovisual detector correctly. Verify: (1) existing CI tests pass without audiovisual deps installed (graceful skip), (2) CI tests pass WITH audiovisual deps installed (uv sync --extra all), (3) pyproject.toml optional-dependencies are correctly structured (audiovisual extra, updated all extra), (4) PyInstaller specs handle the new optional imports without errors (lazy-load pattern works in frozen builds), (5) release workflow builds complete on macOS and Windows. Run the full test suite (uv run pytest) and confirm no regressions.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T22:27:21Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T23:18:59Z","closed_at":"2026-02-24T23:18:59Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-5cw","depends_on_id":"clipshow-ese","type":"blocks","created_at":"2026-02-24T14:27:27Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-5si","title":"Step 18: Emotion Detector","description":"Implement detection/emotion.py: MediaPipe face detection + deepface-onnx emotion classification. Lazy-load on first use. Sample at 3 FPS. Score higher for positive/high-energy emotions (happy, surprise). Show install prompt if deps missing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T23:01:34Z","closed_at":"2026-02-23T23:01:34Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-5si","depends_on_id":"clipshow-052","type":"blocks","created_at":"2026-02-23T14:00:33Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-5us","title":"Plan: substantially improve semantic analysis quality and performance","description":"Write and document a plan for major improvements to the semantic detector. The current implementation uses CLIP ViT-B/32 at 2 FPS with single-frame scoring. This bead tracks a comprehensive improvement plan covering the areas below.\n\n## Current Limitations\n- CLIP ViT-B/32 is a small, older model with limited visual understanding\n- Frame-by-frame processing with zero temporal context (can't detect actions/events)\n- 2 FPS sampling misses brief moments; no batching (slow)\n- Single full-frame embedding — no attention to regions of interest\n- Naive text prompts with no prompt engineering or ensemble techniques\n\n## Proposed Improvement Areas\n\n### 1. Model Upgrade: SigLIP or CLIP ViT-L/14\nReplace ViT-B/32 (~338MB) with a stronger vision-language model:\n- **SigLIP ViT-B/16** (~400MB ONNX) — significantly better zero-shot accuracy than CLIP ViT-B/32, trained with sigmoid loss instead of contrastive, better calibrated scores\n- **CLIP ViT-L/14** (~900MB ONNX) — much larger receptive field and embedding space, best open CLIP variant\n- Keep ViT-B/32 as a \"lite\" fallback for users who want smaller downloads\n- Benchmark: SigLIP ViT-B/16 gets ~70% ImageNet zero-shot vs CLIP ViT-B/32's ~63%\n\n### 2. Temporal Context: Video-Aware Scoring\nCurrent approach scores each frame independently. Improvements:\n- **Sliding window aggregation** — score groups of 3-5 frames and combine embeddings (mean pool) before comparing to text, capturing short actions\n- **Keyframe + context** — for each sampled frame, also sample ±0.5s neighbors and use the max/mean of the group\n- **Temporal attention** — lightweight temporal transformer on top of per-frame CLIP embeddings to model short-range video dynamics (would need a small custom ONNX model)\n- **Optional: X-CLIP or VideoCLIP** — purpose-built video-language models that understand temporal dynamics, though much heavier\n\n### 3. Multi-Scale / Region Crops\nThe current approach embeds the full frame only. Adding region crops can catch small but important subjects:\n- **Center crop + full frame** — embed both and take the max similarity\n- **Adaptive crops** — use a lightweight object detector or saliency map to find regions of interest, then embed those crops separately\n- **Multi-resolution** — process at 224px (standard CLIP) and 336px (higher detail) and combine\n\n### 4. Batch Processing \u0026 GPU Acceleration\nCurrent approach processes 1 frame at a time on CPU:\n- **Batch inference** — collect N frames (e.g., 8-16) and run ONNX inference in a single batch, dramatically reducing per-frame overhead\n- **ONNX GPU provider** — use CUDAExecutionProvider or CoreMLExecutionProvider when available, with automatic fallback to CPU\n- **Async frame decoding** — decode frames in a separate thread while inference runs on the current batch\n\n### 5. Prompt Engineering \u0026 Ensemble\nCurrent prompts are single sentences. Better approaches:\n- **Prompt templates** — use ensemble of templates like \"a photo of {}\", \"a video frame showing {}\", \"a screenshot of {}\" and average their text embeddings (this is what OpenAI recommends for CLIP)\n- **Hierarchical prompts** — break user prompts into categories (action, object, scene, emotion) and weight them differently\n- **Auto-generated negative prompts** — given positive prompts, automatically generate contrasting negatives using antonyms or LLM suggestions\n\n### 6. Adaptive Sampling\nInstead of fixed 2 FPS:\n- **Two-pass approach** — first pass at 1 FPS with cheap features (histogram diff, motion magnitude) to identify \"interesting\" regions, then second pass at 4-8 FPS with full CLIP only on those regions\n- **Score-guided sampling** — if a frame scores high, increase sampling rate in that neighborhood to find precise boundaries\n\n## Implementation Priority\n1. Batch processing (biggest speed win, easy)\n2. Model upgrade to SigLIP (biggest quality win, moderate effort)\n3. Prompt ensemble (free quality win, easy)\n4. Adaptive sampling (speed + quality, moderate)\n5. Multi-scale crops (quality win, moderate)\n6. Temporal context (quality win for action detection, harder)","design":"Plan written at docs/semantic-improvements-plan.md. Six phases: (1) Batch processing — 2-4x speed, batch CLIP inference 16 frames at a time; (2) Prompt ensemble — 5-15% quality, average text embeddings across template variants; (3) SigLIP model upgrade — ~10% quality, replace onnx_clip with direct ONNX Runtime loader and SigLIP ViT-B/16; (4) Adaptive sampling — 2-3x speed, two-pass coarse-then-fine approach; (5) Multi-scale crops — center crop + full frame, max across views; (6) Temporal context — sliding window mean-pool of frame embeddings. Phases 1+2 are quick wins, 3 is the biggest lift, 4-6 build on batching.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:51:39Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T22:15:26Z","closed_at":"2026-02-24T22:15:26Z","close_reason":"Plan written and committed at docs/semantic-improvements-plan.md"}
{"id":"clipshow-5w5","title":"Surface skipped detector warnings and install onnx_clip dependency","description":"The semantic detector silently skips when onnx_clip is missing. Two fixes: 1) Add user-visible feedback when a detector is skipped due to missing dependency. 2) Install onnx_clip and onnxruntime as project dependencies.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T05:08:09Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T05:11:32Z","closed_at":"2026-02-24T05:11:32Z","close_reason":"Closed"}
{"id":"clipshow-6w1","title":"Add temporal smoothing to semantic scores","description":"A brief high-scoring frame surrounded by low ones is likely noise. Apply a small smoothing window to semantic scores to reduce false positives.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:04:08Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:24:29Z","closed_at":"2026-02-24T19:24:29Z","close_reason":"Closed"}
{"id":"clipshow-6yk","title":"Fix stale version strings in packaging files","description":"packaging/clipshow_macos.spec has CFBundleShortVersionString=0.1.0 and packaging/installer.iss has AppVersion=0.1.0 and OutputBaseFilename=ClipShow-0.1.0-setup. Update all to 0.3.0.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:12:07Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:21:45Z","closed_at":"2026-02-24T21:21:45Z","close_reason":"Closed"}
{"id":"clipshow-7lq","title":"AudioVisualDetector implementation","description":"Implement clipshow/detection/audiovisual.py — new Detector subclass that uses LanguageBind ONNX models for joint audio+video+text scoring. Samples video frames and audio windows, encodes via separate ONNX sessions, fuses audio and video similarities (configurable alpha weight) before comparing to text prompts. Supports positive/negative prompt contrast and sigmoid normalization. Note: LanguageBind video encoder expects 8-frame clips, providing natural temporal context.","notes":"TDD: clipshow-7v1 (tests) must be completed first. Implement AudioVisualDetector to make those tests pass. Then run testreview-gemini to confirm coverage.","status":"closed","priority":2,"issue_type":"feature","owner":"kerry@andromeda.net","created_at":"2026-02-24T22:24:52Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T22:59:56Z","closed_at":"2026-02-24T22:59:56Z","close_reason":"AudioVisualDetector fully implemented: frame sampling, video/audio/text encoding, fusion with configurable audio_weight, sigmoid normalization, temporal smoothing. All 24 tests pass.","dependencies":[{"issue_id":"clipshow-7lq","depends_on_id":"clipshow-7v1","type":"blocks","created_at":"2026-02-24T14:28:27Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-7lq","depends_on_id":"clipshow-hph","type":"blocks","created_at":"2026-02-24T14:24:59Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-7lq","depends_on_id":"clipshow-z23","type":"blocks","created_at":"2026-02-24T14:25:00Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-7lq","depends_on_id":"clipshow-z4z","type":"blocks","created_at":"2026-02-24T14:24:59Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-7sh","title":"Step 12: Video Preview","description":"Implement ui/video_preview.py: QMediaPlayer + QVideoWidget wrapper for video playback. Play segment at given start/end time. Play/pause controls. Mock QMediaPlayer in tests to avoid actual playback.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:41:13Z","closed_at":"2026-02-23T22:41:13Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-7sh","depends_on_id":"clipshow-haj","type":"blocks","created_at":"2026-02-23T14:00:30Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-7v1","title":"Unit + integration tests for AudioVisualDetector","description":"TDD: Write tests BEFORE implementing AudioVisualDetector. Define the expected interface and behavior through tests first. Test suite for clipshow/detection/audiovisual.py. Unit tests: audio preprocessing produces correct mel-spectrogram shapes, audio/video similarity fusion math (verify alpha weighting), sigmoid normalization, temporal alignment of audio windows to video frame times, positive/negative prompt contrast scoring. Integration tests: mock all three ONNX sessions with canned embeddings, run full detect() on a synthetic video with audio (reuse conftest.py fixtures), verify DetectorResult shape and score range [0,1]. Test cancellation mid-detect. Test with prompts and negative prompts. Tests will initially fail (red); clipshow-7lq implements to make them pass (green). Run testreview-gemini when done.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T22:27:11Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T22:50:52Z","closed_at":"2026-02-24T22:50:52Z","close_reason":"TDD tests written: 24 tests (8 pass for init/defaults, 16 red for unimplemented detect/load/fusion). Stub audiovisual.py with imports and skeleton.","dependencies":[{"issue_id":"clipshow-7v1","depends_on_id":"clipshow-z23","type":"blocks","created_at":"2026-02-24T14:27:26Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-8eq","title":"Step 14: Review Panel","description":"Implement ui/review_panel.py: left side draggable segment list with include/exclude checkboxes, right side video preview widget. Bottom trim controls with +/-0.5s nudge buttons. Click segment to preview. Implement ui/segment_list.py and ui/timeline_widget.py. Write test_ui_review.py verifying reorder, trim, include/exclude, preview integration.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:44:47Z","closed_at":"2026-02-23T22:44:47Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-8eq","depends_on_id":"clipshow-mtn","type":"blocks","created_at":"2026-02-23T14:00:31Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-8v0","title":"Step 17: Semantic Detector","description":"Implement detection/semantic.py: CLIP-based content scoring via onnx_clip (ONNX Runtime, no PyTorch). Lazy-load on first use with importlib.import_module(). Auto-download CLIP ViT-B/32 model (338MB) to ~/.clipshow/models/. Sample frames at 1 FPS, score against user text prompts, return cosine similarity. Show install/download prompt if deps missing.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T23:01:33Z","closed_at":"2026-02-23T23:01:33Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-8v0","depends_on_id":"clipshow-052","type":"blocks","created_at":"2026-02-23T14:00:32Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-8zn","title":"Step 9: Auto Mode","description":"Wire up run_auto_mode() in app.py: load files, run ffprobe for metadata, run detection pipeline with default settings, take all segments above threshold sorted chronologically, assemble via MoviePy, write output. Support --headless (no GUI) and minimal progress window. Write test_auto_mode.py as full E2E with synthetic videos.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:54:13Z","closed_at":"2026-02-23T22:54:13Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-8zn","depends_on_id":"clipshow-052","type":"blocks","created_at":"2026-02-23T14:00:29Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-8zn","depends_on_id":"clipshow-bhm","type":"blocks","created_at":"2026-02-23T14:00:29Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-9ck","title":"Step 19: Visual Regression Baselines","description":"Regenerate test_ui_layout.py visual regression baselines for all panels. Capture screenshots at fixed 800x600 via widget.grab(). Compare with scikit-image SSIM (threshold 0.95). Support --update-baselines flag. Save diff screenshots as artifacts on CI failure. Cover: import (empty/loaded), analyze (default/custom), review (with segments), export (default/progress), main window.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T23:04:24Z","closed_at":"2026-02-23T23:04:24Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-9ck","depends_on_id":"clipshow-bir","type":"blocks","created_at":"2026-02-23T14:00:33Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-9rm","title":"Step 1: Project Scaffold","description":"- type: task","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:11:53Z","closed_at":"2026-02-23T22:11:53Z","close_reason":"Closed"}
{"id":"clipshow-9tm","title":"Update README to explain lite vs full release builds","description":"Add a section to README explaining the two release build variants: lite (smaller download, downloads CLIP models on first use, needs internet) vs full (larger ~800MB, works offline immediately). Explain why the full builds are large (577MB CLIP ViT-B/32 ONNX model).","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:12:13Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:36:46Z","closed_at":"2026-02-24T21:36:46Z","close_reason":"Closed"}
{"id":"clipshow-ad8","title":"E2E UI test with real fixture videos","description":"Write automated test that drives MainWindow through full workflow (import, analyze, review, export) using real video fixtures from tests/fixtures/videos/","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-23T23:55:00Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T00:01:41Z","closed_at":"2026-02-24T00:01:41Z","close_reason":"Closed"}
{"id":"clipshow-age","title":"Fix Ubuntu CI: missing libEGL.so.1 for Qt offscreen mode","description":"Ubuntu CI fails with INTERNALERROR: ImportError: libEGL.so.1: cannot open shared object file. Need to install libegl1-mesa in the apt-get step of ci.yml.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T00:23:20Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T00:34:43Z","closed_at":"2026-02-24T00:34:43Z","close_reason":"Closed"}
{"id":"clipshow-bgw","title":"Slider help text in analyze panel","description":"Add explanatory help text below detector weights and threshold sliders","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T03:58:25Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T03:59:24Z","closed_at":"2026-02-24T03:59:24Z","close_reason":"Closed"}
{"id":"clipshow-bhm","title":"Step 8: Video Assembler","description":"Implement export/assembler.py: use MoviePy 2.x to concatenate video subclips and encode to output MP4 (H.264). Support subclip start/end times. Hard cuts only (no crossfades). Write test_assembler.py verifying output exists, correct duration, valid codec. Verify via ffprobe.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:29:26Z","closed_at":"2026-02-23T22:29:26Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-bhm","depends_on_id":"clipshow-ocm","type":"blocks","created_at":"2026-02-23T14:00:29Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-bir","title":"Step 16: Full UI E2E","description":"Write test_ui_workflow.py: full end-to-end UI test simulating Import -\u003e Analyze -\u003e Review -\u003e Export journey with qtbot. Programmatically add test videos, trigger analysis, verify segments, toggle/reorder segments, export to temp file, verify valid MP4 output. Run testreview-gemini on all UI tests.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:51:38Z","closed_at":"2026-02-23T22:51:38Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-bir","depends_on_id":"clipshow-3fc","type":"blocks","created_at":"2026-02-23T14:00:32Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-bjn","title":"Show detector category in review panel","description":"Add detectors field to HighlightSegment and show detector tags in segment list","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T03:58:34Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:01:50Z","closed_at":"2026-02-24T04:01:50Z","close_reason":"Closed"}
{"id":"clipshow-di1","title":"Step 20: Packaging \u0026 CI/CD","description":"Create PyInstaller specs (packaging/clipshow_macos.spec, clipshow_windows.spec), Inno Setup script (packaging/installer.iss), release.yml GitHub Action (triggered on version tags). macOS: .app bundle + .dmg via create-dmg. Windows: folder-mode .exe + Inno installer. Both: hiddenimports for PySide6 multimedia + scenedetect submodules. Exclude tkinter/matplotlib.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T23:05:32Z","closed_at":"2026-02-23T23:05:32Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-di1","depends_on_id":"clipshow-bir","type":"blocks","created_at":"2026-02-23T14:00:33Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-dw7","title":"Increase semantic detector sample rate to 2 FPS","description":"Current 1 FPS sampling misses brief highlights (\u003c 1s). Bump SAMPLE_FPS to 2 for better coverage at modest speed cost.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:04:08Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:24:29Z","closed_at":"2026-02-24T19:24:29Z","close_reason":"Closed"}
{"id":"clipshow-ebo","title":"AudioVisualDetector pipeline and UI integration","description":"Register audiovisual detector in pipeline.py (_get_optional_detector, DETECTOR_CLASSES). Add audiovisual_weight and audiovisual_audio_weight to Settings/config.py. Add audiovisual section to YAML config support. Add detector row to analyze panel UI with model download note. Add audiovisual to pyproject.toml optional dependencies.","notes":"TDD: clipshow-ese (E2E tests) must be completed first. Implement pipeline integration to make those tests pass. Must confirm no regressions in existing test suite (uv run pytest). Then clipshow-5cw verifies CI/release builds.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T22:24:55Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T23:17:42Z","closed_at":"2026-02-24T23:17:42Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-ebo","depends_on_id":"clipshow-7lq","type":"blocks","created_at":"2026-02-24T14:25:00Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-ebo","depends_on_id":"clipshow-ese","type":"blocks","created_at":"2026-02-24T14:28:45Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-er5","title":"Feature 21: Semantic Prompts + Pipeline Bug Fix","description":"Fix semantic_prompts not passed to SemanticDetector in pipeline.py. Create PromptEditor widget. Add semantic/emotion rows to AnalyzePanel. Write tests.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-23T23:24:54Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-23T23:27:05Z","closed_at":"2026-02-23T23:27:05Z","close_reason":"Closed"}
{"id":"clipshow-ese","title":"End-to-end pipeline test with AudioVisualDetector","description":"TDD: Write end-to-end tests BEFORE pipeline integration (clipshow-ebo). Integration test that runs the full DetectionPipeline with audiovisual detector enabled alongside existing detectors (scene, audio, motion). Uses synthetic test video with audio from conftest.py. Mocks ONNX model sessions (no real model download). Verifies: audiovisual detector is invoked when weight \u003e 0, skipped when weight = 0 or deps missing, its scores are correctly resampled and combined with other detectors via weighted_combine, final DetectedMoments are produced with audiovisual in contributing_detectors. Also test YAML config loading with audiovisual section. Also verify no regressions — run existing test_pipeline.py and test_detectors.py tests and confirm they still pass unchanged. Tests will initially fail (red); clipshow-ebo implements to make them pass (green).","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T22:27:16Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T23:09:47Z","closed_at":"2026-02-24T23:09:47Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-ese","depends_on_id":"clipshow-7v1","type":"blocks","created_at":"2026-02-24T14:27:27Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-ese","depends_on_id":"clipshow-p6f","type":"blocks","created_at":"2026-02-24T14:27:26Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-ffl","title":"Fix Flatpak bundle: sanitize branch name slash in artifact filename","description":"The Bundle Flatpak step fails because github.ref_name for branches like feature/semantic-analysis-plan contains a slash, creating an invalid path. Need to sanitize the ref name by replacing / with -.","status":"closed","priority":1,"issue_type":"bug","owner":"kerry@andromeda.net","created_at":"2026-02-25T00:49:14Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-25T00:49:46Z","closed_at":"2026-02-25T00:49:46Z","close_reason":"Closed"}
{"id":"clipshow-gkm","title":"Full release builds not bundling ONNX models — full/lite artifacts nearly identical size","description":"The full release builds don't actually bundle ONNX models. Root causes:\n1. onnx_clip's PyPI wheel is only 1.8MB — the CLIP ONNX models (~577MB) are downloaded from S3 at runtime, not included in the package. So collect_data_files('onnx_clip') on CI only collects small metadata files.\n2. emotion-ferplus model (~33MB) is downloaded at runtime and never bundled.\n3. LanguageBind/SigLIP models in MODEL_REGISTRY have placeholder URLs (404) — not yet hosted.\n\nFix: Add a build step that pre-downloads the working models (CLIP image+text, emotion-ferplus) and passes them to PyInstaller as extra data files. Models should be placed where the runtime code expects them (onnx_clip/data/ for CLIP, ~/.clipshow/models/ for emotion).","status":"closed","priority":1,"issue_type":"bug","owner":"kerry@andromeda.net","created_at":"2026-02-25T18:15:19Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-25T18:21:18Z","closed_at":"2026-02-25T18:21:18Z","close_reason":"Closed"}
{"id":"clipshow-haj","title":"Step 10: UI Shell","description":"Implement ui/main_window.py: 4-tab workflow (Import, Analyze, Review, Export) with sequential tab enabling. Navigation buttons (Back/Next). Write test_ui_layout.py baseline screenshots. Verify PySide6 launches headlessly with QT_QPA_PLATFORM=offscreen.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:29:27Z","closed_at":"2026-02-23T22:29:27Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-haj","depends_on_id":"clipshow-9rm","type":"blocks","created_at":"2026-02-23T14:00:29Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-hf4","title":"Add FFmpeg thread parallelism flags","description":"Add -threads 0 to FFmpeg subprocess calls in audio extraction and MoviePy write_videofile to enable multi-threaded encoding/decoding","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T01:16:27Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T01:16:39Z","closed_at":"2026-02-24T01:16:39Z","close_reason":"Closed"}
{"id":"clipshow-hph","title":"ONNX model infrastructure (ModelManager)","description":"Build shared model download/cache/load system in clipshow/detection/models.py. Handles downloading ONNX model files from GitHub releases, caching in ~/.clipshow/models/, creating ort.InferenceSession with auto-detected execution providers (CUDA \u003e CoreML \u003e CPU). Used by both SigLIP (Phase 3) and LanguageBind (Phase 7). Includes progress callbacks for UI download indicators.","notes":"TDD: clipshow-p6f (tests) must be completed first. Implement ModelManager to make those tests pass. Then run testreview-gemini to confirm coverage.","status":"closed","priority":2,"issue_type":"feature","owner":"kerry@andromeda.net","created_at":"2026-02-24T22:24:44Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T22:36:48Z","closed_at":"2026-02-24T22:36:48Z","close_reason":"ModelManager implemented: ensure_model (download/cache with progress), load_session (InferenceSession with auto provider detection), _get_providers (CUDA \u003e CoreML \u003e CPU). All 27 tests pass.","dependencies":[{"issue_id":"clipshow-hph","depends_on_id":"clipshow-p6f","type":"blocks","created_at":"2026-02-24T14:28:26Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-iov","title":"Sort segments by filename in main window","description":"Sort segments by (source_path, start_time) in _on_analysis_complete to match CLI behavior","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T03:58:35Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:01:30Z","closed_at":"2026-02-24T04:01:30Z","close_reason":"Closed"}
{"id":"clipshow-jlm","title":"Fix README cloning instructions referencing 'your-org'","description":"README mentions 'your-org' instead of the actual repo path in cloning instructions. GitHub issue #2.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:04:07Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:05:37Z","closed_at":"2026-02-24T19:05:37Z","close_reason":"Closed"}
{"id":"clipshow-jrn","title":"Replace segment list with table view in review panel","description":"Replace QListWidget with QTableWidget for cleaner columnar display of segment data","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T04:35:12Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:37:23Z","closed_at":"2026-02-24T04:37:23Z","close_reason":"Closed"}
{"id":"clipshow-k94","title":"Better CLIP similarity normalization","description":"Use sigmoid-based normalization centered on typical CLIP match threshold instead of linear (score+1)/2 rescaling. Current approach squashes everything into a narrow band around 0.55-0.68, losing dynamic range between real matches and noise.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:04:07Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:24:28Z","closed_at":"2026-02-24T19:24:28Z","close_reason":"Closed"}
{"id":"clipshow-l78","title":"Block Next until analysis completes","description":"Disable Next button on Analyze tab until analysis produces results","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T03:58:34Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:01:30Z","closed_at":"2026-02-24T04:01:30Z","close_reason":"Closed"}
{"id":"clipshow-lns","title":"Feature 23: README","description":"Write comprehensive README.md for novice users covering all features.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-23T23:24:56Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-23T23:29:33Z","closed_at":"2026-02-23T23:29:33Z","close_reason":"Closed"}
{"id":"clipshow-mtn","title":"Step 13: Analyze Panel","description":"Implement ui/analyze_panel.py: sliders for detector weights and threshold, enable/disable checkboxes per detector. Semantic detectors show model download note. Per-file progress bars. Analyze All button launches AnalysisWorker (QThread). Write workers/analysis_worker.py. Write test_ui_analyze.py verifying slider binding, worker signals, progress, cancel.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:42:59Z","closed_at":"2026-02-23T22:42:59Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-mtn","depends_on_id":"clipshow-052","type":"blocks","created_at":"2026-02-23T14:00:31Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-mtn","depends_on_id":"clipshow-7sh","type":"blocks","created_at":"2026-02-23T14:00:31Z","created_by":"Kerry Scharfglass","metadata":"{}"},{"issue_id":"clipshow-mtn","depends_on_id":"clipshow-uzs","type":"blocks","created_at":"2026-02-23T14:00:30Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-n8b","title":"Update README to promote release builds over pip install","description":"Adjust the README installation section to direct users towards downloading pre-built release builds (DMG, Windows installer, Flatpak) as the primary install method, with pip/uv install as a secondary developer option.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T20:41:45Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T20:43:26Z","closed_at":"2026-02-24T20:43:26Z","close_reason":"Closed"}
{"id":"clipshow-noq","title":"Parallel video analysis with ThreadPoolExecutor","description":"Use ThreadPoolExecutor in AnalysisWorker to analyze multiple videos concurrently, keeping granular per-file progress","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T04:44:56Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:49:32Z","closed_at":"2026-02-24T04:49:32Z","close_reason":"Closed"}
{"id":"clipshow-nr3","title":"Step 4: Scene Detector","description":"Implement detection/scene.py: wrap PySceneDetect ContentDetector for HSV-weighted frame differencing. Implement Detector interface from base.py. Return DetectorResult with normalized [0,1] scores per timestep. Spikes at cuts/transitions. Add tests to test_detectors.py verifying scene change detection on the scene_change_video fixture.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:25:54Z","closed_at":"2026-02-23T22:25:54Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-nr3","depends_on_id":"clipshow-144","type":"blocks","created_at":"2026-02-23T14:00:27Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-o5b","title":"Step 5: Audio Detector","description":"Implement detection/audio.py: extract audio to temp WAV via FFmpeg subprocess, run librosa onset strength + RMS energy analysis. Implement Detector interface. Return DetectorResult with normalized scores capturing loud moments, speech, beats. Add tests to test_detectors.py verifying detection on loud_moment_video fixture.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:25:54Z","closed_at":"2026-02-23T22:25:54Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-o5b","depends_on_id":"clipshow-144","type":"blocks","created_at":"2026-02-23T14:00:27Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-ocm","title":"Step 2: ffprobe Module","description":"Implement export/ffprobe.py: extract video metadata (duration, width, height, fps, codec) by calling ffprobe as a subprocess. Parse JSON output. Populate VideoSource fields. Write test_ffprobe.py verifying extraction on synthetic test videos from conftest.py.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:19:16Z","closed_at":"2026-02-23T22:19:16Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-ocm","depends_on_id":"clipshow-9rm","type":"blocks","created_at":"2026-02-23T14:00:26Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-p6f","title":"Unit + integration tests for ModelManager","description":"TDD: Write tests BEFORE implementing ModelManager. Define the expected interface and behavior through tests first. Test suite for clipshow/detection/models.py. Unit tests: download with mocked HTTP (verify cache path, retry on failure, progress callback), load_session with mocked ort.InferenceSession (verify provider selection logic: CUDA \u003e CoreML \u003e CPU), cache hit skips download, corrupt/incomplete file re-downloads. Integration test: load a tiny dummy ONNX file from a local fixture, verify InferenceSession creation. All tests must mock network calls — no real downloads in CI. Tests will initially fail (red); clipshow-hph implements to make them pass (green).","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T22:27:06Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T22:35:21Z","closed_at":"2026-02-24T22:35:21Z","close_reason":"TDD tests written: 29 tests (7 pass for registry/init, 20 red for unimplemented methods, 2 integration skipped). Stub models.py created with MODEL_REGISTRY and ModelManager skeleton."}
{"id":"clipshow-qnk","title":"Processing rate, ETA, and frame preview during analysis","description":"Add rate/ETA display and frame preview thumbnail during video analysis","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T03:58:29Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:00:58Z","closed_at":"2026-02-24T04:00:58Z","close_reason":"Closed"}
{"id":"clipshow-sci","title":"Fix Windows release build: emotion detector race condition and error handling","description":"The Windows full release build fails on test_multiple_files_accepted because auto mode enables the emotion detector (weight 0.2), and ProcessPoolExecutor spawns workers that concurrently download emotion-ferplus-8.onnx, causing a race condition (EACCES). Fix: 1) Use atomic download in emotion.py 2) Catch Exception (not just RuntimeError) in pipeline.py 3) Handle subprocess exceptions in _analyze_all_parallel","status":"closed","priority":1,"issue_type":"bug","owner":"kerry@andromeda.net","created_at":"2026-02-25T00:46:58Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-25T00:48:27Z","closed_at":"2026-02-25T00:48:27Z","close_reason":"Closed"}
{"id":"clipshow-t1a","title":"macOS app bundle crashes: missing imageio package metadata","description":"The macOS .app bundle fails at launch with 'importlib.metadata.PackageNotFoundError: No package metadata was found for imageio'. The chain is: moviepy -\u003e moviepy.config -\u003e imageio.__init__ which calls importlib.metadata.version('imageio'). PyInstaller is not including imageio's dist-info metadata. Fix: add imageio to copy_metadata() in the macOS spec, or add a PyInstaller hook.","status":"closed","priority":1,"issue_type":"bug","owner":"kerry@andromeda.net","created_at":"2026-02-25T18:18:52Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-25T18:21:18Z","closed_at":"2026-02-25T18:21:18Z","close_reason":"Closed"}
{"id":"clipshow-tah","title":"Default all analysis detectors to unchecked in UI","description":"Change the Analyze panel so all detector checkboxes start unchecked (weight=0) by default, requiring users to explicitly enable the detectors they want. Currently scene/audio/motion/emotion are enabled by default which can be confusing for new users.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:50:56Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:56:18Z","closed_at":"2026-02-24T21:56:18Z","close_reason":"Closed"}
{"id":"clipshow-tws","title":"Add macOS smoke test: download and launch release build","description":"Add a step to the release verification process that downloads the macOS .dmg artifact and runs a basic smoke test (launch the app, verify it starts without crashing). This could be a manual checklist item or an automated post-release CI job.","status":"open","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-25T18:19:37Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-25T18:19:37Z"}
{"id":"clipshow-uyp","title":"Expose negative prompts in UI with clear-all button","description":"Add negative prompts list to the Edit Prompts dialog alongside positive prompts. Add a Clear All button that empties both lists. Add semantic_negative_prompts field to Settings.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:17:05Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:27:31Z","closed_at":"2026-02-24T19:27:31Z","close_reason":"Closed"}
{"id":"clipshow-uzs","title":"Step 11: Import Panel","description":"Implement ui/import_panel.py: drag-drop area + file browser button. File list showing filename, duration, resolution. Remove/clear buttons. Next button disabled when no files. Write test_ui_import.py verifying add/remove/clear flows and button states.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2026-02-23T21:59:31Z","updated_at":"2026-02-23T22:40:04Z","closed_at":"2026-02-23T22:40:04Z","close_reason":"Closed","dependencies":[{"issue_id":"clipshow-uzs","depends_on_id":"clipshow-haj","type":"blocks","created_at":"2026-02-23T14:00:30Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-v5c","title":"Fix GH release build failures (macOS + Linux)","description":"macOS: PyInstaller fails with 'not a fat binary' because target_arch=universal2 but numpy is arm64-only. Fix: change target_arch to None (native). Linux: Flatpak fails because pip --no-index --find-links=vendor has no vendor dir with setuptools. Fix: allow network access during build or bundle deps.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:28:25Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T20:00:11Z","closed_at":"2026-02-24T20:00:11Z","close_reason":"Closed"}
{"id":"clipshow-vsp","title":"Add auto-balance toggle for detector weights","description":"Add a toggle on the analysis screen that enables a mode where enabled detector weights automatically divide evenly from 100%. When one detector is enabled it defaults to 100%, two detectors default to 50% each, three to ~33% each, etc. When a detector is toggled on/off in this mode, the remaining enabled detectors should automatically rebalance.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T04:48:40Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:51:06Z","closed_at":"2026-02-24T04:51:06Z","close_reason":"Closed"}
{"id":"clipshow-vyk","title":"Fix scene detector non-deterministic set iteration bug","description":"SceneDetector iterated over StatsManager metric_keys (a set) to find the score key. Due to Python hash randomization, different processes pick different keys from the set. If delta_hue is picked instead of content_val, scores are all-zero for black→white transitions. Fixed by using ContentDetector.FRAME_SCORE_KEY directly.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T02:43:00Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T02:43:29Z","closed_at":"2026-02-24T02:43:29Z","close_reason":"Closed"}
{"id":"clipshow-wb2","title":"Parallel video processing: add --workers flag and ProcessPoolExecutor","description":"Add max_workers to Settings, parallelize analyze_all() with ProcessPoolExecutor, add --workers/-j CLI flag, update settings dialog, and add tests.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-23T23:48:48Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-23T23:52:27Z","closed_at":"2026-02-23T23:52:27Z","close_reason":"Closed"}
{"id":"clipshow-wd0","title":"Add YAML pipeline configuration file support for batch processing","description":"Allow users to pass a YAML config file (e.g. --config pipeline.yaml) to configure the full detection pipeline: detector weights, thresholds, semantic prompts, output settings, input file globs, etc. Useful for repeatable batch processing. Include YAML format documentation in the README with a complete example.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:50:58Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T22:06:05Z","closed_at":"2026-02-24T22:06:05Z","close_reason":"Closed"}
{"id":"clipshow-wkp","title":"Rework analyze panel layout with splitter and file status list","description":"Use QSplitter for settings vs progress area, replace frame preview with file list showing per-clip processing status","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T04:32:34Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T04:34:19Z","closed_at":"2026-02-24T04:34:19Z","close_reason":"Closed"}
{"id":"clipshow-xxu","title":"Add negative prompts for semantic detector","description":"Score frames against both positive prompts ('exciting moment') and negative prompts ('boring static shot'), use the difference. This dramatically improves contrast between interesting and uninteresting frames.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:04:08Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:24:29Z","closed_at":"2026-02-24T19:24:29Z","close_reason":"Closed"}
{"id":"clipshow-ylu","title":"Fix emotion detector: broken deepface_onnx and mediapipe API","description":"emotion.py imports deepface_onnx but it's not on PyPI (pyproject.toml wrongly lists deepface which is a different TF-based package). Also mediapipe 0.10.31+ removed mp.solutions API. Needs rewrite: either pin mediapipe\u003c0.10.31 and replace deepface with a working ONNX emotion model, or migrate to new MediaPipe Tasks API.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T21:12:10Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T21:35:43Z","closed_at":"2026-02-24T21:35:43Z","close_reason":"Closed"}
{"id":"clipshow-z23","title":"AudioVisualDetector: audio preprocessing","description":"Implement audio feature extraction for LanguageBind in the AudioVisualDetector. Extract audio from video via ffmpeg (reuse AudioDetector pattern), split into 2-second windows aligned to video sample times, compute 128-band mel-spectrograms using librosa (already a dependency). Output shape: (T, 128, freq_bins) matching LanguageBind audio encoder input format.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T22:24:49Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T22:48:58Z","closed_at":"2026-02-24T22:48:58Z","close_reason":"Audio preprocessing module with extract_audio_wav, compute_mel_spectrogram, extract_mel_windows, and format_for_languagebind. 19 tests all passing.","dependencies":[{"issue_id":"clipshow-z23","depends_on_id":"clipshow-z4z","type":"blocks","created_at":"2026-02-24T14:24:59Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-z4z","title":"LanguageBind ONNX export script","description":"Write scripts/export_languagebind.py — a one-time developer script (requires PyTorch + LanguageBind) that exports the three LanguageBind encoders (video, audio, text) to ONNX format. Export only the transformer encoders; preprocessing stays in Python. Include validation that compares ONNX output to PyTorch output on reference inputs. FT (fine-tuned) model variants preferred. Upload exported ONNX files to a GitHub release.","status":"closed","priority":2,"issue_type":"task","owner":"kerry@andromeda.net","created_at":"2026-02-24T22:24:47Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T22:46:31Z","closed_at":"2026-02-24T22:46:31Z","close_reason":"Export script written with video/audio/text encoder wrappers, validation against PyTorch outputs, CLI with --encoder/--validate-only/--opset-version flags.","dependencies":[{"issue_id":"clipshow-z4z","depends_on_id":"clipshow-hph","type":"blocks","created_at":"2026-02-24T14:24:58Z","created_by":"Kerry Scharfglass","metadata":"{}"}]}
{"id":"clipshow-z6x","title":"Fix release process: disable flaky tests, attach partial artifacts","description":"Release CI exits with errors due to flaky tests (MoviePy BrokenPipeError on Windows, intermittent fixture failures). Disable known flaky tests and ensure partial build artifacts are still attached even if one platform fails. GitHub issue #1.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","owner":"kerry@andromeda.net","created_at":"2026-02-24T19:04:07Z","created_by":"Kerry Scharfglass","updated_at":"2026-02-24T19:09:31Z","closed_at":"2026-02-24T19:09:31Z","close_reason":"Closed"}
